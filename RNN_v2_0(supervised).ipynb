{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN v2_0(supervised).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojn2000/DeepProject/blob/main/RNN_v2_0(supervised).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQcwPFm4rS84",
        "outputId": "3673892a-67d3-4a72-8a19-02ed8758e599"
      },
      "source": [
        "!pip3 install pickle5\n",
        "import pickle5 as pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = 'drive/My Drive/Skole/Uni/9_semester/02456_deep_learning/Project/'\n",
        "\n",
        "if (not(os.path.exists(drive_path))):\n",
        "  drive_path = 'drive/MyDrive/DL project/'  # Idas path    \n",
        "\n",
        "f0 = \"padded_data.pickle\"\n",
        "\n",
        "## LOAD PADDED and CLEANED data \n",
        "with open(drive_path + f0, 'rb') as f:\n",
        "    data = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 92 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 133 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 153 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 163 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 174 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 184 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 194 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 204 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 215 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 225 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 235 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 245 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 11.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B22Tj0P2cVTb"
      },
      "source": [
        "# Load functions\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, leaky_relu, linear \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dZYr_0ztq6b"
      },
      "source": [
        "Separate into test, train, and bumpy roads "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkJzeAN3tye9"
      },
      "source": [
        "# Definition of bumpy road in IRI terms \n",
        "tau = 4.5 \n",
        "\n",
        "good_data = data[data['IRI_mean'] <= tau]\n",
        "bad_data = data[data['IRI_mean'] > tau]\n",
        "\n",
        "# Separate good_data into test and train sets \n",
        "good_train, good_test = train_test_split(good_data, test_size=0.2)\n",
        "bad_train, bad_test = train_test_split(bad_data, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZQBroJnxqux"
      },
      "source": [
        "# Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZW_EyvxcppL"
      },
      "source": [
        "train_data = []\n",
        "test_data = []\n",
        "test_bad = []\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "# Test data\n",
        "for i in range(len(good_test['GM.acc.xyz.z'])):\n",
        "   x_data = good_test['GM.acc.xyz.z'].values[i].flatten() \n",
        "   label = 1\n",
        "   test_data.append([x_data, label])\n",
        "\n",
        "for i in range(len(bad_test['GM.acc.xyz.z'])):\n",
        "   x_data = bad_test['GM.acc.xyz.z'].values[i].flatten() \n",
        "   label = 0\n",
        "   test_data.append([x_data, label])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# Train data\n",
        "for i in range(len(good_train['GM.acc.xyz.z'])):\n",
        "   x_data = good_train['GM.acc.xyz.z'].values[i].flatten()\n",
        "   label = 1\n",
        "   train_data.append([x_data, label])\n",
        "\n",
        "for i in range(len(bad_train['GM.acc.xyz.z'])):\n",
        "   x_data = bad_train['GM.acc.xyz.z'].values[i].flatten() \n",
        "   label = 0\n",
        "   train_data.append([x_data, label])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkFAwBFytIjT"
      },
      "source": [
        "# RNN network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lulqvXEtHiw",
        "outputId": "3b0490a5-3c21-4ba1-f830-2409c90920fe"
      },
      "source": [
        "# Define size variables\n",
        "\n",
        "num_features = len(good_train['GM.acc.xyz.z'].values[0])\n",
        "hidden_units = 32 \n",
        "latent_features = 8 \n",
        "layer_dim = 2\n",
        "dropout_prob = 0 \n",
        "\n",
        "\n",
        "class Supervised(nn.Module):\n",
        "    def __init__(self, hidden_units, latent_features):\n",
        "        super(Supervised, self).__init__()\n",
        "\n",
        "        # ENCODER\n",
        "        #self.rnn_in = nn.GRU(num_features,(hidden_units*2), layer_dim, batch_first=True, dropout=dropout_prob)\n",
        "        #self.linear_in1 = nn.Linear(in_features=(hidden_units*2), out_features=hidden_units)\n",
        "        #self.linear_in2 = nn.Linear(in_features=hidden_units, out_features=latent_features)        \n",
        "        #self.linear_last = nn.Linear(in_features=latent_features, out_features=1)\n",
        "        \n",
        "        self.rnn_in = nn.GRU(num_features,(hidden_units), layer_dim, batch_first=True, dropout=dropout_prob)\n",
        "        self.linear_in2 = nn.Linear(in_features=hidden_units, out_features=latent_features)        \n",
        "        self.linear_last = nn.Linear(in_features=latent_features, out_features=1)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x): \n",
        "        outputs = {}\n",
        "        \n",
        "        # RNN\n",
        "        x = x[None,:, :]\n",
        "        x, h = self.rnn_in(x)\n",
        "\n",
        "        # LINEAR\n",
        "        x = x[0, :, :]\n",
        "        #x = relu(self.linear_in1(tanh(x)))\n",
        "        #x = self.linear_in2(x)\n",
        "\n",
        "        #x = relu(self.linear_in1(tanh(x)))\n",
        "        x = self.linear_in2(tanh(x))\n",
        "        \n",
        "\n",
        "        x_hat = (tanh(self.linear_last(x))+1)/2\n",
        "        #x_hat = sigmoid(self.linear_last(x))\n",
        "\n",
        "        return {\n",
        "            'x_hat': x_hat\n",
        "        }\n",
        "\n",
        "\n",
        "# Choose the shape of the autoencoder\n",
        "net = Supervised(hidden_units=hidden_units, latent_features=latent_features)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised(\n",
            "  (rnn_in): GRU(3366, 32, num_layers=2, batch_first=True)\n",
            "  (linear_in2): Linear(in_features=32, out_features=8, bias=True)\n",
            "  (linear_last): Linear(in_features=8, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4r-4BTeyO8R"
      },
      "source": [
        "## Set network parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGqTXspN5hHk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#OPTIMIZER\n",
        "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Cross entropyloss\n",
        "def loss_function(yHat, y):\n",
        "    return ((torch.sum(-torch.log(x_hat[y==1])) + 8*torch.sum(-torch.log(1-x_hat[y==0])))/len(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ys17NlylEA"
      },
      "source": [
        "## Train the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAJCt2886Cuz"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "nRuns = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV8rxalFynLu",
        "outputId": "55dfb88a-16d8-454c-9246-4126e561f628"
      },
      "source": [
        "# EPOCHS\n",
        "num_epochs = 30\n",
        "\n",
        "# TRAIN MODEL\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    batch_loss = []\n",
        "    net.train()\n",
        "    \n",
        "    for x,y in train_loader:\n",
        "        x = x.float()\n",
        "        outputs = net(x)\n",
        "        x_hat = outputs['x_hat']\n",
        "\n",
        "        # LOSS\n",
        "        loss = loss_function(x_hat, y)\n",
        "        \n",
        "        # UPDATE MODEL\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "    train_loss.append(np.mean(batch_loss))\n",
        "\n",
        "    # Evaluate, do not propagate gradients\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "      \n",
        "        x, y = next(iter(test_loader))\n",
        "        x = x.float()\n",
        "        outputs = net(x)\n",
        "\n",
        "        x_hat = outputs['x_hat']\n",
        "       # z = outputs['z'].cpu().numpy()\n",
        "\n",
        "        loss = loss_function(x_hat, y[:,None].float())\n",
        "\n",
        "        valid_loss.append(loss.item())\n",
        "    \n",
        "    if epoch == 0:\n",
        "        continue\n",
        "\n",
        "nRuns += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n",
            "Epoch:  2\n",
            "Epoch:  3\n",
            "Epoch:  4\n",
            "Epoch:  5\n",
            "Epoch:  6\n",
            "Epoch:  7\n",
            "Epoch:  8\n",
            "Epoch:  9\n",
            "Epoch:  10\n",
            "Epoch:  11\n",
            "Epoch:  12\n",
            "Epoch:  13\n",
            "Epoch:  14\n",
            "Epoch:  15\n",
            "Epoch:  16\n",
            "Epoch:  17\n",
            "Epoch:  18\n",
            "Epoch:  19\n",
            "Epoch:  20\n",
            "Epoch:  21\n",
            "Epoch:  22\n",
            "Epoch:  23\n",
            "Epoch:  24\n",
            "Epoch:  25\n",
            "Epoch:  26\n",
            "Epoch:  27\n",
            "Epoch:  28\n",
            "Epoch:  29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAczrzL2kLbR"
      },
      "source": [
        "# PLOT TRAINING CURVE \n",
        "import matplotlib.pyplot as plt\n",
        "len_epoch = range(len(train_loss))\n",
        "\n",
        "\n",
        "plt.figure(dpi=800)\n",
        "plt.plot(len_epoch,np.log10(train_loss),color='r',label=\"Train loss\")\n",
        "plt.plot(len_epoch,np.log10(valid_loss),color='b', label= \"Validation loss\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE error\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cffI7qJ82ovm",
        "outputId": "93647900-446d-4698-fb94-937a7c21f3ad"
      },
      "source": [
        "torch.mean(x_hat[y==0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4682)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NSZ44cBwT8l",
        "outputId": "64f582b1-a6bd-4dad-9aec-7f819b260266"
      },
      "source": [
        "prob_good = []\n",
        "prob_bad = []\n",
        "\n",
        "for x,y in train_loader:\n",
        "  x = x.float()\n",
        "  outputs = net(x)\n",
        "  x_hat = outputs['x_hat']\n",
        "  prob_good.extend(x_hat[y==1].flatten().tolist())\n",
        "  prob_bad.extend(x_hat[y==0].flatten().tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqgSO8lH5SrG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpEg7uRc1e00",
        "outputId": "84df78b5-f36d-4b47-a7e2-3aae08666500"
      },
      "source": [
        "eps = 1e-6\n",
        "tau_min = min(min(prob_good),min(prob_bad)) - eps\n",
        "tau_max = max(max(prob_good),max(prob_bad)) + eps\n",
        "print(tau_min)\n",
        "print(tau_max)\n",
        "\n",
        "N = 100\n",
        "\n",
        "perf = np.array([np.zeros(N), np.zeros(N), np.zeros(N), np.zeros(N), np.zeros(N)])\n",
        "ii = 0\n",
        "for tau in np.linspace(tau_min, tau_max, num = N):\n",
        "  perf[0,ii] = tau\n",
        "  perf[1,ii] = sum(prob_good>tau)  # true positives\n",
        "  perf[2,ii] = sum(prob_good<=tau) # false positive \n",
        "  perf[3,ii] = sum(prob_bad>tau)   # false negatives\n",
        "  perf[4,ii] = sum(prob_bad<=tau) # true negatives\n",
        "  ii += 1\n",
        "\n",
        "perf = pd.DataFrame(perf)\n",
        "perf.to_pickle(drive_path+'RNN_v2_0.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18677921430969238\n",
            "0.6259317861328125\n"
          ]
        }
      ]
    }
  ]
}