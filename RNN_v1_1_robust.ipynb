{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN v1_1-robust.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojn2000/DeepProject/blob/main/RNN_v1_1_robust.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQcwPFm4rS84",
        "outputId": "e25f65a1-021b-4039-d3f5-ec6de9cfb4b2"
      },
      "source": [
        "!pip3 install pickle5\n",
        "import pickle5 as pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = 'drive/My Drive/Skole/Uni/9_semester/02456_deep_learning/Project/'\n",
        "\n",
        "if (not(os.path.exists(drive_path))):\n",
        "  drive_path = 'drive/MyDrive/DL project/'  # Idas path    \n",
        "\n",
        "f0 = \"padded_data.pickle\"\n",
        "\n",
        "## LOAD PADDED and CLEANED data \n",
        "with open(drive_path + f0, 'rb') as f:\n",
        "    data = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 92 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 133 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 153 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 163 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 174 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 184 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 194 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 204 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 215 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 225 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 235 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 245 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 12.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B22Tj0P2cVTb"
      },
      "source": [
        "# Load functions\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, leaky_relu, linear \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dZYr_0ztq6b"
      },
      "source": [
        "Separate into test, train, and bumpy roads "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkJzeAN3tye9"
      },
      "source": [
        "# Definition of bumpy road in IRI terms \n",
        "tau = 4.5 \n",
        "\n",
        "good_data = data[data['IRI_mean'] <= tau]\n",
        "bad_data = data[data['IRI_mean'] > tau]\n",
        "\n",
        "\n",
        "# Separate good_data into test and train sets \n",
        "n = 2500 # sample size of train set\n",
        "\n",
        "good_train, good_test = train_test_split(good_data, test_size=0.2)\n",
        "#bad_train, bad_test = train_test_split(bad_data, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZQBroJnxqux"
      },
      "source": [
        "# Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZW_EyvxcppL",
        "outputId": "6d5552cb-0f3e-40b6-b781-bafe48c0828e"
      },
      "source": [
        "train_data = []\n",
        "test_data = []\n",
        "test_bad = []\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# test data\n",
        "for i in range(len(good_test['GM.acc.xyz.z'])):\n",
        "   #x_data = np.concatenate((good_test['GM.acc.xyz.z'].values[i].flatten(),good_test['GM.obd.spd_veh.value'].values[i].flatten()),axis=0)\n",
        "   x_data = good_test['GM.acc.xyz.z'].values[i].flatten()\n",
        "   labels = [i] * len(x_data)\n",
        "   test_data.append([x_data, labels])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# train data\n",
        "for i in range(len(good_train['GM.acc.xyz.z'])):\n",
        "   #x_data = np.concatenate((good_train['GM.acc.xyz.z'].values[i].flatten(),good_train['GM.obd.spd_veh.value'].values[i].flatten()),axis=0)\n",
        "   x_data = good_train['GM.acc.xyz.z'].values[i].flatten()\n",
        "   labels = [i] * len(x_data)\n",
        "   train_data.append([x_data, labels])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# bad data\n",
        "for i in range(len(bad_data['GM.acc.xyz.z'])):\n",
        "   #x_data = np.concatenate((bad_data['GM.acc.xyz.z'].values[i].flatten(),bad_data['GM.obd.spd_veh.value'].values[i].flatten()),axis=0)\n",
        "   x_data = bad_data['GM.acc.xyz.z'].values[i].flatten()\n",
        "   labels = [i] * len(x_data)\n",
        "   test_bad.append([x_data, labels])\n",
        "\n",
        "bad_loader = torch.utils.data.DataLoader(test_bad, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "i1, l1 = next(iter(train_loader))\n",
        "print(i1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3366])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkFAwBFytIjT"
      },
      "source": [
        "# RNN network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lulqvXEtHiw",
        "outputId": "f91f995d-c270-407c-ac7a-107f9e3830c5"
      },
      "source": [
        "# define size variables\n",
        "# num_features = len(good_train['GM.acc.xyz.z'].values[0]) + len(good_train['GM.obd.spd_veh.value'].values[0])\n",
        "num_features = len(good_train['GM.acc.xyz.z'].values[0])\n",
        "hidden_units = 24 #40\n",
        "latent_features = 8 #8\n",
        "\n",
        "seq_len = 1\n",
        "n_features = num_features\n",
        "latent_dim = latent_features\n",
        "hidden_size = hidden_units\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, n_features, latent_dim, hidden_size):\n",
        "      super(EncoderRNN, self).__init__()\n",
        "\n",
        "      self.n_features = n_features\n",
        "      self.hidden_size = hidden_size\n",
        "      self.latent_dim = latent_dim\n",
        "\n",
        "      self.gru_enc = nn.GRU(n_features, hidden_size,\n",
        "                        batch_first = True,dropout=0, # tried dropout, not effecttive\n",
        "                        bidirectional=True)\n",
        "      \n",
        "      self.lat_layer = nn.GRU(hidden_size*2, latent_dim,\n",
        "                        batch_first = True, dropout=0,\n",
        "                        bidirectional = False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x, _  = self.gru_enc(x)\n",
        "      x , h = self.lat_layer(x)\n",
        "      return x[:,-1].unsqueeze(1)\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, seq_len, n_features , latent_dim , hidden_size):\n",
        "      super(DecoderRNN, self).__init__()\n",
        "\n",
        "      self.seq_len = seq_len\n",
        "      self.n_features = n_features\n",
        "      self.latent_dim = latent_dim\n",
        "      self.hidden_size = hidden_size\n",
        "\n",
        "      self.gru_dec1 = nn.GRU(latent_dim, latent_dim,\n",
        "                      batch_first = True, dropout=0,\n",
        "                      bidirectional= False)\n",
        "\n",
        "      self.gru_dec2 = nn.GRU(latent_dim, hidden_size,\n",
        "                        batch_first = True, dropout=0,\n",
        "                        bidirectional= True)\n",
        "\n",
        "      self.output_layer = nn.Linear(self.hidden_size*2, n_features,bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = x.repeat(1,self.seq_len, 1)\n",
        "      x, _ = self.gru_dec1(x)\n",
        "      x, _ = self.gru_dec2(x)\n",
        "      return (self.output_layer(x))\n",
        "\n",
        "\n",
        "class AERNN(nn.Module):\n",
        "    def __init__(self, seq_len, n_features, latent_dim , hidden_size):\n",
        "      super(AERNN, self).__init__()\n",
        "\n",
        "      self.seq_len = seq_len\n",
        "      self.encoder = EncoderRNN(n_features, latent_dim, hidden_size)\n",
        "      self.decoder = DecoderRNN(seq_len, n_features, latent_dim, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = self.encoder(x)\n",
        "      x = self.decoder(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "net = AERNN(seq_len, n_features, latent_dim , hidden_size)\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AERNN(\n",
            "  (encoder): EncoderRNN(\n",
            "    (gru_enc): GRU(3366, 24, batch_first=True, bidirectional=True)\n",
            "    (lat_layer): GRU(48, 8, batch_first=True)\n",
            "  )\n",
            "  (decoder): DecoderRNN(\n",
            "    (gru_dec1): GRU(8, 8, batch_first=True)\n",
            "    (gru_dec2): GRU(8, 24, batch_first=True, bidirectional=True)\n",
            "    (output_layer): Linear(in_features=48, out_features=3366, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4r-4BTeyO8R"
      },
      "source": [
        "## Set network parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq8wL0IhfWTB",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import os\n",
        "from typing import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from IPython.display import Image, display, clear_output\n",
        "from sklearn.manifold import TSNE\n",
        "from torch import Tensor\n",
        "from torch.distributions import Normal\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "def plot_autoencoder_stats(\n",
        "        x: Tensor = None,\n",
        "        x_hat: Tensor = None,\n",
        "        z: Tensor = None,\n",
        "        y: Tensor = None,\n",
        "        epoch: int = None,\n",
        "        train_loss: List = None,\n",
        "        valid_loss: List = None,\n",
        "        classes: List = None,\n",
        "        dimensionality_reduction_op: Optional[Callable] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    An utility \n",
        "    \"\"\"\n",
        "    # -- Plotting --\n",
        "    f, axarr = plt.subplots(2, 2, figsize=(20, 20))\n",
        "\n",
        "    # Loss\n",
        "    ax = axarr[0, 0]\n",
        "    ax.set_title(\"Error\")\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Error')\n",
        "\n",
        "    ax.plot(np.arange(epoch + 1), train_loss, color=\"black\")\n",
        "    ax.plot(np.arange(epoch + 1), valid_loss, color=\"gray\", linestyle=\"--\")\n",
        "    ax.legend(['Training error', 'Validation error'])\n",
        "\n",
        "    # Latent space\n",
        "    ax = axarr[0, 1]\n",
        "\n",
        "    ax.set_title('Latent space')\n",
        "    ax.set_xlabel('Dimension 1')\n",
        "    ax.set_ylabel('Dimension 2')\n",
        "\n",
        "    # If you want to use a dimensionality reduction method you can use\n",
        "    # for example TSNE by projecting on two principal dimensions\n",
        "    # TSNE.fit_transform(z)\n",
        "    if dimensionality_reduction_op is not None:\n",
        "        z = dimensionality_reduction_op(z)\n",
        "\n",
        "    colors = iter(plt.get_cmap('Set1')(np.linspace(0, 1.0, len(classes))))\n",
        "    for c in classes:\n",
        "        ax.scatter(*z[y.numpy() == c].T, c=next(colors), marker='o')\n",
        "\n",
        "    ax.legend(classes)\n",
        "\n",
        "    # Inputs\n",
        "    ax = axarr[1, 0]\n",
        "    ax.set_title('Inputs')\n",
        "    ax.axis('off')\n",
        "\n",
        "    rows = 8\n",
        "    batch_size = x.size(0)\n",
        "    columns = batch_size // rows\n",
        "\n",
        "    canvas = np.zeros((28 * rows, columns * 28))\n",
        "    for i in range(rows):\n",
        "        for j in range(columns):\n",
        "            idx = i % columns + rows * j\n",
        "            canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = x[idx].reshape((28, 28))\n",
        "    ax.imshow(canvas, cmap='gray')\n",
        "\n",
        "    # Reconstructions\n",
        "    ax = axarr[1, 1]\n",
        "    ax.set_title('Reconstructions')\n",
        "    ax.axis('off')\n",
        "\n",
        "    canvas = np.zeros((28 * rows, columns * 28))\n",
        "    for i in range(rows):\n",
        "        for j in range(columns):\n",
        "            idx = i % columns + rows * j\n",
        "            canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = x_hat[idx].reshape((28, 28))\n",
        "\n",
        "    ax.imshow(canvas, cmap='gray')\n",
        "\n",
        "    tmp_img = \"tmp_ae_out.png\"\n",
        "    plt.savefig(tmp_img)\n",
        "    plt.close(f)\n",
        "    display(Image(filename=tmp_img))\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    os.remove(tmp_img)\n",
        "\n",
        "\n",
        "def plot_samples(ax, x):\n",
        "    x = x.to('cpu')\n",
        "    nrow = int(np.sqrt(x.size(0)))\n",
        "    x_grid = make_grid(x.view(-1, 1, 28, 28), nrow=nrow).permute(1, 2, 0)\n",
        "    ax.imshow(x_grid)\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_interpolations(ax, vae):\n",
        "    device = next(iter(vae.parameters())).device\n",
        "    nrow = 10\n",
        "    nsteps = 10\n",
        "    prior_params = vae.prior_params.expand(2 * nrow, *vae.prior_params.shape[-1:])\n",
        "    mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
        "    pz = Normal(mu, log_sigma.exp())\n",
        "    z = pz.sample().view(nrow, 2, -1)\n",
        "    t = torch.linspace(0, 1, 10, device=device)\n",
        "    zs = t[None, :, None] * z[:, 0, None, :] + (1 - t[None, :, None]) * z[:, 1, None, :]\n",
        "    px = vae.observation_model(zs.view(nrow * nsteps, -1))\n",
        "    x = px.sample()\n",
        "    x = x.to('cpu')\n",
        "    x_grid = make_grid(x.view(-1, 1, 28, 28), nrow=nrow).permute(1, 2, 0)\n",
        "    ax.imshow(x_grid)\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_grid(ax, vae):\n",
        "    device = next(iter(vae.parameters())).device\n",
        "    nrow = 10\n",
        "    xv, yv = torch.meshgrid([torch.linspace(-3, 3, 10), torch.linspace(-3, 3, 10)])\n",
        "    zs = torch.cat([xv[:, :, None], yv[:, :, None]], -1)\n",
        "    zs = zs.to(device)\n",
        "    px = vae.observation_model(zs.view(nrow * nrow, 2))\n",
        "    x = px.sample()\n",
        "    x = x.to('cpu')\n",
        "    x_grid = make_grid(x.view(-1, 1, 28, 28), nrow=nrow).permute(1, 2, 0)\n",
        "    ax.imshow(x_grid)\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_2d_latents(ax, qz, z, y):\n",
        "    z = z.to('cpu')\n",
        "    y = y.to('cpu')\n",
        "    scale_factor = 2\n",
        "    batch_size = z.shape[0]\n",
        "    palette = sns.color_palette()\n",
        "    colors = [palette[l] for l in y]\n",
        "\n",
        "    # plot prior\n",
        "    prior = plt.Circle((0, 0), scale_factor, color='gray', fill=True, alpha=0.1)\n",
        "    ax.add_artist(prior)\n",
        "\n",
        "    # plot data points\n",
        "    mus, sigmas = qz.mu.to('cpu'), qz.sigma.to('cpu')\n",
        "    mus = [mus[i].numpy().tolist() for i in range(batch_size)]\n",
        "    sigmas = [sigmas[i].numpy().tolist() for i in range(batch_size)]\n",
        "\n",
        "    posteriors = [\n",
        "        plt.matplotlib.patches.Ellipse(mus[i], *(scale_factor * s for s in sigmas[i]), color=colors[i], fill=False,\n",
        "                                       alpha=0.3) for i in range(batch_size)]\n",
        "    for p in posteriors:\n",
        "        ax.add_artist(p)\n",
        "\n",
        "    ax.scatter(z[:, 0], z[:, 1], color=colors)\n",
        "\n",
        "    ax.set_xlim([-3, 3])\n",
        "    ax.set_ylim([-3, 3])\n",
        "    ax.set_aspect('equal', 'box')\n",
        "\n",
        "\n",
        "def plot_latents(ax, z, y):\n",
        "    z = z.to('cpu')\n",
        "    palette = sns.color_palette()\n",
        "    colors = [palette[l] for l in y]\n",
        "    z = TSNE(n_components=2).fit_transform(z)\n",
        "    ax.scatter(z[:, 0], z[:, 1], color=colors)\n",
        "\n",
        "\n",
        "def make_vae_plots(vae, x, y, outputs, training_data, validation_data, tmp_img=\"tmp_vae_out.png\", figsize=(18, 18)):\n",
        "    fig, axes = plt.subplots(3, 3, figsize=figsize, squeeze=False)\n",
        "\n",
        "    # plot the observation\n",
        "    axes[0, 0].set_title(r'Observation $\\mathbf{x}$')\n",
        "    plot_samples(axes[0, 0], x)\n",
        "\n",
        "    # plot the latent samples\n",
        "    try:\n",
        "        z = outputs['z']\n",
        "        if z.shape[1] == 2:\n",
        "            axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$')\n",
        "            qz = outputs['qz']\n",
        "            plot_2d_latents(axes[0, 1], qz, z, y)\n",
        "        else:\n",
        "            axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ (t-SNE)')\n",
        "            plot_latents(axes[0, 1], z, y)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate the plot of the latent sanples because of exception\")\n",
        "        print(e)\n",
        "\n",
        "    # plot posterior samples\n",
        "    axes[0, 2].set_title(\n",
        "        r'Reconstruction $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$')\n",
        "    px = outputs['px']\n",
        "    x_sample = px.sample().to('cpu')\n",
        "    plot_samples(axes[0, 2], x_sample)\n",
        "\n",
        "    # plot ELBO\n",
        "    ax = axes[1, 0]\n",
        "    ax.set_title(r'ELBO: $\\mathcal{L} ( \\mathbf{x} )$')\n",
        "    ax.plot(training_data['elbo'], label='Training')\n",
        "    ax.plot(validation_data['elbo'], label='Validation')\n",
        "    ax.legend()\n",
        "\n",
        "    # plot KL\n",
        "    ax = axes[1, 1]\n",
        "    ax.set_title(r'$\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)$')\n",
        "    ax.plot(training_data['kl'], label='Training')\n",
        "    ax.plot(validation_data['kl'], label='Validation')\n",
        "    ax.legend()\n",
        "\n",
        "    # plot NLL\n",
        "    ax = axes[1, 2]\n",
        "    ax.set_title(r'$\\log p_\\theta(\\mathbf{x} | \\mathbf{z})$')\n",
        "    ax.plot(training_data['log_px'], label='Training')\n",
        "    ax.plot(validation_data['log_px'], label='Validation')\n",
        "    ax.legend()\n",
        "\n",
        "    # plot prior samples\n",
        "    axes[2, 0].set_title(r'Samples $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim p(\\mathbf{z})$')\n",
        "    px = vae.sample_from_prior(batch_size=x.size(0))['px']\n",
        "    x_samples = px.sample()\n",
        "    plot_samples(axes[2, 0], x_samples)\n",
        "\n",
        "    # plot interpolations samples\n",
        "    axes[2, 1].set_title(\n",
        "        r'Latent Interpolations: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | t \\cdot \\mathbf{z}_1 + (1-t) \\cdot \\mathbf{z}_2), \\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z}), t=0 \\dots 1$')\n",
        "    plot_interpolations(axes[2, 1], vae)\n",
        "\n",
        "    # plot samples (sampling from a grid instead of the prior)\n",
        "    if vae.latent_features == 2:\n",
        "        axes[2, 2].set_title(\n",
        "            r'Samples: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim \\operatorname{grid}(-3:3, -3:3)$')\n",
        "        px = vae.sample_from_prior(batch_size=x.size(0))['px']\n",
        "        x_samples = px.sample()\n",
        "        plot_grid(axes[2, 2], vae)\n",
        "\n",
        "    # display\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(tmp_img)\n",
        "    plt.close(fig)\n",
        "    display(Image(filename=tmp_img))\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    os.remove(tmp_img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGqTXspN5hHk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# if you want L2 regularization, then add weight_decay to SGD\n",
        "#optimizer = optim.Adam(net.parameters(), lr=1e-3,weight_decay=1e-4) #weight_decay=1e-2\n",
        "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3,weight_decay=1e-4) #weight_decay=1e-4\n",
        "\n",
        "# custom loss function\n",
        "def loss_function(x_hat,x):\n",
        "\n",
        "    x_hat[x[:,:,:] == 0] = 0 \n",
        "\n",
        "    #loss = torch.diagonal(torch.matmul(x_hat[:,0,:] - x[:,0,:],torch.transpose(x_hat[:,0,:] - x[:,0,:] ,0,1))) / torch.sum(x[:,0,:] != 0, dim = 1)\n",
        "    loss = torch.sum((torch.abs(x_hat[:,0,:] - x[:,0,:])), dim = 1) / torch.sum(x[:,0,:] != 0, dim = 1)\n",
        "\n",
        "    return(sum(loss)/batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ys17NlylEA"
      },
      "source": [
        "## Train the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAJCt2886Cuz"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV8rxalFynLu"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    batch_loss = []\n",
        "    net.train()\n",
        "    \n",
        "    # Go through each batch in the training dataset using the loader\n",
        "    # Note that y is not necessarily known as it is here\n",
        "    for x,y in train_loader:\n",
        "        \n",
        "        x = x.float()\n",
        "        #print(x.shape)\n",
        "        x = x[:,None,:]\n",
        "        outputs = net(x)\n",
        "        x_hat = outputs\n",
        "        #x_hat = outputs['x_hat']\n",
        "        #x_hat = outputs[:,0,:]\n",
        "        #x = x[:,0,:]\n",
        "        #x_hat[x[:,:] == 0] = 0\n",
        "\n",
        "        # note, target is the original tensor, as we're working with auto-encoders\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss = loss_function(x_hat, x)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "    train_loss.append(np.mean(batch_loss))\n",
        "\n",
        "    # Evaluate, do not propagate gradients\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        \n",
        "        # Just load a single batch from the test loader\n",
        "        x, y = next(iter(test_loader))\n",
        "        \n",
        "        x = x.float()\n",
        "        x = x[:,None,:]\n",
        "        outputs = net(x)\n",
        "        x_hat = outputs\n",
        "\n",
        "        # We save the latent variable and reconstruction for later use\n",
        "        # we will need them on the CPU to plot\n",
        "        #x_hat = outputs['x_hat']\n",
        "        #x_hat[x[:,:] == 0] = 0\n",
        "\n",
        "        #z = outputs['z'].cpu().numpy()\n",
        "\n",
        "        loss = loss_function(x_hat, x)\n",
        "\n",
        "        valid_loss.append(loss.item())\n",
        "    \n",
        "    if epoch == 0:\n",
        "        continue\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd2mKQZGvps9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAczrzL2kLbR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "len_epoch = range(len(train_loss))\n",
        "#len_epoch = len_epoch[0:19]\n",
        "\n",
        "plt.plot(len_epoch,np.log10(train_loss),color='r',label=\"Train loss\")\n",
        "plt.plot(len_epoch,np.log10(valid_loss),color='b', label= \"Validation loss\")\n",
        "#plt.plot(len_epoch,train_loss,color='r',label=\"Train loss\")\n",
        "#plt.plot(len_epoch,valid_loss,color='b', label= \"Validation loss\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE error\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyq6VCNx8DzL"
      },
      "source": [
        "sqerr_good = []\n",
        "for x,y in test_loader:\n",
        "  x = x.float()\n",
        "  x = x[:,None,:]\n",
        "  outputs = net(x)\n",
        "  x_hat = outputs#['x_hat']\n",
        "  x_hat[x==0] = 0\n",
        "  #sqerr_good.extend((torch.diagonal(torch.matmul(x_hat[:,0,:] - x[:,0,:],torch.transpose(x_hat[:,0,:] - x[:,0,:] ,0,1))) / torch.sum(x[:,0,:] != 0, dim = 1)).tolist())\n",
        "  sqerr_good.extend((torch.sum((torch.abs(x_hat[:,0,:] - x[:,0,:])), dim = 1) / torch.sum(x[:,0,:] != 0, dim = 1) / torch.sum(x[:,0,:] != 0, dim = 1)).tolist())\n",
        "  #for ii in range(x_hat.shape[0]):\n",
        "  #  sqerr_good.append(float(sum((x[ii,0,x[ii,0,:] != 0] - x_hat[ii,0,x[ii,0,:] != 0])**2)) / sum(x[ii,0,:] != 0))\n",
        "\n",
        "plt.hist(sqerr_good, bins = 40)\n",
        "plt.title(\"Test data\")\n",
        "plt.show()\n",
        "\n",
        "#print(x.shape)\n",
        "#print(x_hat.shape)\n",
        "#print(loss_function(x,x_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbmEM8v3DJCY"
      },
      "source": [
        "sqerr_bad = []\n",
        "for x,y in bad_loader:\n",
        "  x = x.float()\n",
        "  x = x[:,None,:]\n",
        "  outputs = net(x)\n",
        "  x_hat = outputs#['x_hat']\n",
        "  x_hat[x == 0] = 0\n",
        "  sqerr_bad.extend((torch.sum((torch.abs(x_hat[:,0,:] - x[:,0,:])), dim = 1) / torch.sum(x[:,0,:] != 0, dim = 1) / torch.sum(x[:,0,:] != 0, dim = 1)).tolist())\n",
        "  #for ii in range(x_hat.shape[0]):\n",
        "    #sqerr_bad.append(float(sum((x[ii,0,x[ii,0,:] != 0] - x_hat[ii,0,x[ii,0,:] != 0])**2)) / sum(x[ii,0,:] != 0))\n",
        "    \n",
        "\n",
        "\n",
        "plt.hist(sqerr_bad, bins = 40)\n",
        "plt.title(\"Bad data\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4PmtRYt1HK8"
      },
      "source": [
        "plt.hist(sqerr_good, bins = 30, alpha=0.5, label='Good')\n",
        "plt.hist(sqerr_bad, bins = 30, alpha=0.5, label='Bad')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Network test for good and bad data\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_RXAZ1DEP8n"
      },
      "source": [
        "min(sqerr_good)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drCXxb9avq9W"
      },
      "source": [
        "#Compute performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V56oFscXjSiK"
      },
      "source": [
        "Tried weight decay -> funky minimum \n",
        "Adam -> not as good as RSMprop\n",
        "Dropout -> also, no performance increase "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdV0OgTkvuu5"
      },
      "source": [
        "from math import log10\n",
        "\n",
        "eps = 1e-6\n",
        "tau_min = min(min(sqerr_good),min(sqerr_bad)) - eps\n",
        "tau_max = max(max(sqerr_good),max(sqerr_bad)) + eps\n",
        "print(tau_min)\n",
        "print(tau_max)\n",
        "\n",
        "N = 100\n",
        "\n",
        "perf = np.array([np.zeros(N), np.zeros(N), np.zeros(N), np.zeros(N), np.zeros(N)])\n",
        "ii = 0\n",
        "for tau in np.logspace(log10(tau_min), log10(tau_max), num = N):\n",
        "  perf[0,ii] = tau\n",
        "  perf[1,ii] = sum(sqerr_good<tau)  # true positives\n",
        "  perf[2,ii] = sum(sqerr_good>=tau) # false positive \n",
        "  perf[3,ii] = sum(sqerr_bad<tau)   # false negatives\n",
        "  perf[4,ii] = sum(sqerr_bad>=tau) # true negatives\n",
        "  ii += 1\n",
        "\n",
        "perf = pd.DataFrame(perf)\n",
        "perf.to_pickle(drive_path+'RNN_v1_1-robust.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0brNla02yAqd"
      },
      "source": [
        "def tpr(mat):\n",
        "  return (perf[1,:]/(perf[1,:]+perf[2,:]))  # TP / (TP+FP)\n",
        "\n",
        "def tnr(mat):\n",
        "  return (perf[4,:]/(perf[3,:]+perf[4,:]))  # TN / (TN+FN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUz5PBdg7WUZ"
      },
      "source": [
        "perf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WunLXr4S-Eck"
      },
      "source": [
        "perf = perf.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGtutWV516EI"
      },
      "source": [
        "plt.plot(tpr(perf),tnr(perf), marker=\".\")\n",
        "plt.plot([0,1],[1,0],linestyle = 'dashed')\n",
        "plt.xlabel('True positive rate', fontsize = 16)\n",
        "plt.ylabel('True negative rate',fontsize = 16)\n",
        "plt.title('Performance curve', fontsize = 20)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBissKJ28Tq"
      },
      "source": [
        "aux = perf[:,16]\n",
        "import seaborn as sn\n",
        "\n",
        "array = [[aux[1]/(aux[1]+aux[2]), aux[2]/(aux[1]+aux[2])],\n",
        "         [aux[3]/(aux[3]+aux[4]), aux[4]/(aux[3]+aux[4])] ]\n",
        "#array = array/(sum(aux[1:4]))*100\n",
        "\n",
        "df_cm = pd.DataFrame(array, ['Good', 'Bad'], ['Good', 'Bad'])\n",
        "sn.set(font_scale=1.2) # for label size\n",
        "sn.heatmap(df_cm, annot=True, cmap = sn.cm.rocket_r, annot_kws={\"size\": 16},vmin = 0, ) # font size\n",
        "plt.xlabel('Predicted',fontsize = 16)\n",
        "plt.ylabel('Reference',fontsize = 16)\n",
        "plt.title('Confusion matrix', fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkwPSZDt7Tq6"
      },
      "source": [
        "aux"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}